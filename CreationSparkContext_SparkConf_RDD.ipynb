{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a62f91d2",
   "metadata": {},
   "source": [
    "# Spark\n",
    "\n",
    "Apache Spark works in a master-slave architecture where the master is called “Driver” and slaves are called “Workers”. The starting point of your Spark application is sc, a Spark Context Class instance. It runs inside the driver.\n",
    "\n",
    "### What is RDD?\n",
    "The Spark Core implementation is a RDD (Resilient Distributed Dataset) which is a collection of distributed data across different nodes of the cluster that are processed in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c340896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show locaton of the spark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7f2c5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47c382f",
   "metadata": {},
   "source": [
    "# How to create SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74ee8ad",
   "metadata": {},
   "source": [
    "## Method 1: SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e0cc900",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "master : Pyspark working mode (local, k8s, yarn) \n",
    "[4] : Number of executors(If you use * it will use all executors)\n",
    "appName: Name of the aplication\n",
    "config(\"spark.executor.memory\", \"4g\")\\ # Servers that do the work 4g: 4 GB \n",
    "config(\"spark.driver.memory\", \"2g\")\\ # The server where the user sends the code and returns the results.\n",
    "getOrCreate : If there is a session, bring it, if not, create a new one.\n",
    "\"\"\"\n",
    "\n",
    "pyspark = SparkSession.builder \\\n",
    ".master(\"local[4]\") \\\n",
    ".appName(\"RDD-Example\") \\\n",
    ".config(\"spark.executor.memory\", \"4g\") \\\n",
    ".config(\"spark.driver.memory\", \"2g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e593492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Spark Context\n",
    "sc = pyspark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "320602ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a rdd\n",
    "rdd1 = sc.parallelize([(\"Ahmet\", 25), (\"Cemal\", 29), (\"Inci\", 38), (\"Burcu\", 33)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a94b44cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ahmet', 25), ('Cemal', 29)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting 2 examples for context\n",
    "rdd1.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cadccbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop session\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b668890b",
   "metadata": {},
   "source": [
    "## Method 2: ParkSession and SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dccc2830",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf() \\\n",
    ".setMaster(\"local[4]\") \\\n",
    ".setAppName(\"RDD-Ex-Method2\") \\\n",
    ".setExecutorEnv(\"spark.executor.memory\", \"4g\") \\\n",
    ".setExecutorEnv(\"spark.executor.memory\", \"4g\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "795d6608",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark = SparkSession.builder\\\n",
    ".config(conf=conf)\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "575ee628",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40543871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a rdd\n",
    "rdd2 = sc.parallelize([(\"Ahmet\", 25), (\"Cemal\", 29), (\"Inci\", 38), (\"Burcu\", 33)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "006313ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ahmet', 25), ('Cemal', 29)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting 2 examples for context\n",
    "rdd2.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f18bcac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop session\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd7cbf5",
   "metadata": {},
   "source": [
    "## Method 3: SparkContext and SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "795c094c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf() \\\n",
    ".setMaster(\"local[4]\") \\\n",
    ".setAppName(\"RDD-Ex-Method2\") \\\n",
    ".setExecutorEnv(\"spark.executor.memory\", \"4g\") \\\n",
    ".setExecutorEnv(\"spark.executor.memory\", \"4g\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1993abe3",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.lang.Thread.run(Thread.java:748)\r\n\tat org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2456)\r\n\tat org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2452)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2452)\r\n\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2541)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:84)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-66f9c693822e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[1;32m--> 118\u001b[1;33m                           conf, jsc, profiler_cls)\n\u001b[0m\u001b[0;32m    119\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m         \u001b[1;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mSparkContext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mallow\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0minitialization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \"\"\"\n\u001b[1;32m--> 282\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1523\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1524\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1525\u001b[1;33m             answer, self._gateway_client, None, self._fqn)\n\u001b[0m\u001b[0;32m   1526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1527\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.lang.Thread.run(Thread.java:748)\r\n\tat org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2456)\r\n\tat org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2452)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2452)\r\n\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2541)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:84)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fc17cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a rdd - tuple\n",
    "rdd3 = sc.parallelize([(\"Ahmet\", 25), (\"Cemal\", 29), (\"Inci\", 38), (\"Burcu\", 33)])\n",
    "\n",
    "#-list\n",
    "rdd4 = sc.parallelize([[\"Ahmet\", 25], [\"Cemal\", 29], [\"Inci\", 38], [\"Burcu\", 33], [\"Faki\", 42]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "556b9278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ahmet', 25), ('Cemal', 29)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting 2 examples for context\n",
    "rdd3.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a74923b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5c135a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Ahmet', 25], ['Cemal', 29]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c538838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af9dc0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [4, 5, 6]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating RDD with integer list\n",
    "numbers_rdd1 = sc.parallelize([[1, 2, 3], [4, 5, 6]])\n",
    "numbers_rdd1.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a95a6d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop session\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "327ec045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0bc9b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark = SparkSession.builder \\\n",
    ".master(\"local[4]\") \\\n",
    ".appName(\"RDD-Example\") \\\n",
    ".config(\"spark.executor.memory\", \"4g\") \\\n",
    ".config(\"spark.driver.memory\", \"2g\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ffe9bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating RDD with dictionary\n",
    "friends_dict = {\n",
    "    \"Name\": [\"Kerem\", \"Fatma\", \"Akif\"],\n",
    "    \"Age\": [40, 42, 18]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cfd896fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kerem</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fatma</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Akif</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Name  Age\n",
       "0  Kerem   40\n",
       "1  Fatma   42\n",
       "2   Akif   18"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(friends_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d60ae704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Kerem| 40|\n",
      "|Fatma| 42|\n",
      "| Akif| 18|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd_df_pandas = pyspark.createDataFrame(df)\n",
    "rdd_df_pandas.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e058f56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Kerem', Age=40),\n",
       " Row(Name='Fatma', Age=42),\n",
       " Row(Name='Akif', Age=18)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_df = rdd_df_pandas.rdd\n",
    "rdd_df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "446afd0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ömer Seyfettin - Forsa',\n",
       " '',\n",
       " 'Akdeniz’in, kahramanlık yuvası sonsuz ufuklarına bakan küçük tepe, minimini bir çiçek ormanı gibiydi. İnce uzun dallı badem ağaçlarının alaca gölgeleri sahile inen keçiyoluna düşüyor, ilkbaharın tatlı rüzgârıyla sarhoş olan martılar, çılgın bağrışlarıyla havayı çınlatıyordu. Badem bahçesinin yanı geniş bir bağdı. Beyaz taşlardan yapılmış kısa bir duvarın ötesindeki harabe vadiye kadar iniyordu. Bağın ortasındaki yıkık kulübenin kapısız girişinden bir ihtiyar çıktı. Saçı sakalı bembeyazdı. Kamburunu düzeltmek istiyormuş gibi gerindi. Elleri, ayakları titriyordu. Gök kadar boş, gök kadar sakin duran denize baktı, baktı.',\n",
       " '',\n",
       " '– Hayırdır inşallah! dedi.',\n",
       " '',\n",
       " 'Duvarın dibindeki taş yığınlarına çöktü. Başını ellerinin arasına aldı. Sırtında yırtık bir çuval vardı. Çıplak ayakları topraktan yoğrulmuş gibiydi. Zayıf kolları kirli tunç rengindeydi. Yine başını kaldırdı. Gökle denizin birleştiği dumandan çizgiye dikkatle baktı, Ama görünürde bir şey yoktu.',\n",
       " '',\n",
       " 'Bu, her gece uykusunda onu kurtarmak için birçok geminin pupa yelken geldiğini gören zavallı eski bir Türk forsasıydı. Tutsak olalı kırk yılı geçmişti. Otuz yaşında, dinç, levent, güçlü bir kahramanken Malta korsanlarının eline düşmüştü. Yirmi yıl onların kadırgalarında kürek çekti. Yirmi yıl iki zincirle iki ayağından rutubetli bir geminin dibine bağlanmış yaşadı. Yirmi yılın yazları, kışları, rüzgârları, fırtınaları, güneşleri onun granit vücudunu eritemedi. Zincirleri küflendi, çürüdü, kırıldı. Yirmi yıl içinde birkaç kez halkalarını, çivilerini değiştirdiler. Ama onun çelikten daha sert kaslı bacaklarına bir şey olmadı. Yalnız aptes alamadığı için. üzülüyordu. Hep güneşin doğduğu yanı sol ilerisine alır, gözlerini kıbleye çevirir, beş vakit namazı gizli işaretle yerine getirirdi.',\n",
       " '']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"omer_seyfettin_forsa_hikaye.txt\"\n",
    "sc = pyspark.sparkContext\n",
    "txt_file = sc.textFile(file_path)\n",
    "txt_file.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e41f9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
